{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关心每一层的变化\n",
    "\n",
    "我发现，MLP和LeNet过滤之后的数据的betti number 表现出的规律性远远超出了data本身的规律性，这是令人难以理解的，有两种方法可以解释：\n",
    "1. data中，`dataloader`加载数据的时候，没有对每一个图片进行单独的增强，而是所有的图片是同样的增强\n",
    "2. `LeNet\\MLP`这样的网络确实大大的提取了数据的高维信息，所以反映出了更好的规律性\n",
    "\n",
    "根据目前的信息来看，第一种的概率是很小的，所以我需要在第二种的假设的基础上进行实验，来观察是不是网络在产生作用，"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 首先做一些测试\n",
    "\n",
    "测试确定我可以正确的得到每一层`forward`的`output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4867, -0.2654, -0.5861,  0.8512,  0.5210,  0.2591,  1.1168,  0.0397,\n",
      "          0.8878,  0.8383,  0.6029,  0.0833, -0.1011, -0.1284,  0.5959,  0.0568,\n",
      "          0.9053, -0.8213,  1.2861,  0.5938]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.4297,  0.2590, -0.4412, -0.0247,  0.5788, -0.1795, -0.1013,  0.2112,\n",
      "          0.5665, -0.6485, -0.2779,  0.5817, -0.6824, -0.0286, -0.2177, -0.8587,\n",
      "          0.2731, -0.3415, -0.3272, -0.1022, -0.7625,  0.0580, -0.3150, -0.1322,\n",
      "         -0.1135, -0.2101,  0.0320, -0.2436,  0.1177, -0.3311]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1211]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 钩子函数，在每一层的forward过程中被调用\n",
    "def hook_function(module, input, output):\n",
    "    module.layer_outputs = output\n",
    "\n",
    "# 定义你的神经网络类\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 20)  # 第一层\n",
    "        self.layer2 = nn.Linear(20, 30)  # 第二层\n",
    "        self.layer3 = nn.Linear(30, 1)   # 最后一层\n",
    "\n",
    "        # 为每一层注册钩子\n",
    "        self.layer1.register_forward_hook(hook_function)\n",
    "        self.layer2.register_forward_hook(hook_function)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过每一层\n",
    "        output1 = self.layer1(x)\n",
    "        output2 = self.layer2(output1)\n",
    "        output3 = self.layer3(output2)\n",
    "\n",
    "        return output3\n",
    "\n",
    "# 创建神经网络对象\n",
    "net = MyNetwork()\n",
    "\n",
    "# 定义输入数据\n",
    "x = torch.randn(1, 10)  # 假设输入数据维度为1x10\n",
    "\n",
    "# 前向传播，并获取每一层的输出\n",
    "output = net(x)\n",
    "\n",
    "# 获取每一层的输出\n",
    "output1 = net.layer1.layer_outputs\n",
    "output2 = net.layer2.layer_outputs\n",
    "output3 = output\n",
    "\n",
    "print(output1)  # 第一层的输出\n",
    "print(output2)  # 第二层的输出\n",
    "print(output3)  # 最后一层的输出"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 然后调用已经准备好的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "这里是得到距离矩阵，是否使用cuda=True\n",
      "这里是得到距离矩阵，是否使用cuda=True\n",
      "./care_layers_output/LeNet/0/\n",
      "这里是得到距离矩阵，是否使用cuda=True\n",
      "这里是得到距离矩阵，是否使用cuda=True\n",
      "./care_layers_output/LeNet/1/\n",
      "这里是得到距离矩阵，是否使用cuda=True\n",
      "这里是得到距离矩阵，是否使用cuda=True\n",
      "./care_layers_output/LeNet/2/\n",
      "这里是得到距离矩阵，是否使用cuda=True\n",
      "这里是得到距离矩阵，是否使用cuda=True\n",
      "./care_layers_output/LeNet/3/\n",
      "这里是得到距离矩阵，是否使用cuda=True\n",
      "这里是得到距离矩阵，是否使用cuda=True\n",
      "./care_layers_output/LeNet/4/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LeNet-birth-death-l1_distance': [array([[   0.     ,  960.89386, 1074.8125 , ..., 1193.0536 ,  980.4014 ,\n",
       "          1154.012  ],\n",
       "         [ 960.89386,    0.     ,  972.02264, ...,  827.58704,  690.9939 ,\n",
       "          1166.8242 ],\n",
       "         [1074.8125 ,  972.02264,    0.     , ...,  995.8995 ,  721.7788 ,\n",
       "          1284.6969 ],\n",
       "         ...,\n",
       "         [1193.0536 ,  827.58704,  995.8995 , ...,    0.     ,  745.2766 ,\n",
       "          1449.9578 ],\n",
       "         [ 980.4014 ,  690.9939 ,  721.7788 , ...,  745.2766 ,    0.     ,\n",
       "          1248.9598 ],\n",
       "         [1154.012  , 1166.8242 , 1284.6969 , ..., 1449.9578 , 1248.9598 ,\n",
       "             0.     ]], dtype=float32),\n",
       "  array([[  0.     , 331.79623, 382.87616, ..., 405.39395, 353.69464,\n",
       "          418.0963 ],\n",
       "         [331.79623,   0.     , 246.99892, ..., 267.02734, 140.0346 ,\n",
       "          457.00464],\n",
       "         [382.87616, 246.99892,   0.     , ..., 315.7141 , 203.61604,\n",
       "          464.69687],\n",
       "         ...,\n",
       "         [405.39395, 267.02734, 315.7141 , ...,   0.     , 239.84811,\n",
       "          570.3259 ],\n",
       "         [353.69464, 140.0346 , 203.61604, ..., 239.84811,   0.     ,\n",
       "          501.87088],\n",
       "         [418.0963 , 457.00464, 464.69687, ..., 570.3259 , 501.87088,\n",
       "            0.     ]], dtype=float32),\n",
       "  array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
       "  array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
       "  array([[ 0.      ,  9.435684,  9.900312, ..., 14.144339, 12.176161,\n",
       "           9.555237],\n",
       "         [ 9.435684,  0.      ,  7.674019, ..., 12.006601,  6.730613,\n",
       "           9.116074],\n",
       "         [ 9.900312,  7.674019,  0.      , ...,  9.539593,  5.340925,\n",
       "          13.626694],\n",
       "         ...,\n",
       "         [14.144339, 12.006601,  9.539593, ...,  0.      , 10.014602,\n",
       "          16.55041 ],\n",
       "         [12.176161,  6.730613,  5.340925, ..., 10.014602,  0.      ,\n",
       "          15.2837  ],\n",
       "         [ 9.555237,  9.116074, 13.626694, ..., 16.55041 , 15.2837  ,\n",
       "           0.      ]], dtype=float32)],\n",
       " 'LeNet-birth-death-l2_distance': [array([[0.0000000e+00, 4.3579670e+01, 4.6409290e+01, ..., 5.1750851e+01,\n",
       "          4.4653400e+01, 5.0924213e+01],\n",
       "         [4.3579674e+01, 1.1048543e-02, 4.1493374e+01, ..., 3.6450371e+01,\n",
       "          3.0104309e+01, 4.9255451e+01],\n",
       "         [4.6409290e+01, 4.1493370e+01, 0.0000000e+00, ..., 4.4120735e+01,\n",
       "          3.3417145e+01, 5.2833130e+01],\n",
       "         ...,\n",
       "         [5.1750851e+01, 3.6450371e+01, 4.4120735e+01, ..., 1.1048543e-02,\n",
       "          3.4586346e+01, 5.9774979e+01],\n",
       "         [4.4653400e+01, 3.0104309e+01, 3.3417145e+01, ..., 3.4586346e+01,\n",
       "          2.0669932e-02, 5.3847439e+01],\n",
       "         [5.0924213e+01, 4.9255451e+01, 5.2833130e+01, ..., 5.9774979e+01,\n",
       "          5.3847439e+01, 0.0000000e+00]], dtype=float32),\n",
       "  array([[7.8125000e-03, 2.6849306e+01, 2.8340565e+01, ..., 3.0179802e+01,\n",
       "          2.7773319e+01, 4.1481163e+01],\n",
       "         [2.6849306e+01, 9.5683197e-03, 1.9747389e+01, ..., 2.1409113e+01,\n",
       "          1.2203486e+01, 4.3193760e+01],\n",
       "         [2.8340565e+01, 1.9747389e+01, 0.0000000e+00, ..., 2.4511173e+01,\n",
       "          1.6722330e+01, 4.3117180e+01],\n",
       "         ...,\n",
       "         [3.0179802e+01, 2.1409113e+01, 2.4511173e+01, ..., 1.2352647e-02,\n",
       "          1.9601040e+01, 4.7382122e+01],\n",
       "         [2.7773319e+01, 1.2203486e+01, 1.6722330e+01, ..., 1.9601040e+01,\n",
       "          0.0000000e+00, 4.4853218e+01],\n",
       "         [4.1481163e+01, 4.3193760e+01, 4.3117184e+01, ..., 4.7382122e+01,\n",
       "          4.4853218e+01, 2.2097087e-02]], dtype=float32),\n",
       "  array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
       "  array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
       "  array([[0.       , 3.895588 , 3.8068585, ..., 5.502824 , 4.8455124,\n",
       "          3.6933765],\n",
       "         [3.895588 , 0.       , 2.986406 , ..., 4.691886 , 2.8758276,\n",
       "          3.4490616],\n",
       "         [3.8068585, 2.986406 , 0.       , ..., 3.3379667, 2.0328069,\n",
       "          5.092017 ],\n",
       "         ...,\n",
       "         [5.502824 , 4.691886 , 3.3379667, ..., 0.       , 3.9992836,\n",
       "          6.23639  ],\n",
       "         [4.8455124, 2.8758276, 2.0328069, ..., 3.9992836, 0.       ,\n",
       "          5.6246595],\n",
       "         [3.6933765, 3.4490614, 5.092017 , ..., 6.23639  , 5.6246595,\n",
       "          0.       ]], dtype=float32)]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset.check_betti4net import get_layer_output_betti\n",
    "from net.custome_net import LeNet, MLP\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "model_LeNet = LeNet()\n",
    "get_layer_output_betti(model=model_LeNet, seed=15, save_root=\"./care_layers_output\", debug_size=1000, name=\"LeNet\")\n",
    "\n",
    "model_LeNet = MLP()\n",
    "get_layer_output_betti(model=model_LeNet, seed=15, save_root=\"./care_layers_output\", debug_size=1000, name=\"MLP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchGpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
