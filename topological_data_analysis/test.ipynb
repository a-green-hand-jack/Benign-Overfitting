{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 experiments_results 是你的三层嵌套列表，这里用一个示例列表表示\n",
    "experiments_results = [\n",
    "    [[1, 2, 3], [4, 5, 6]],\n",
    "    [[7, 8, 9], [10, 11, 12]],\n",
    "    [[13, 14, 15], [16, 17, 18]],\n",
    "    [[19, 20, 21], [22, 23, 24]],\n",
    "    [[25, 26, 27], [28, 29, 30]],\n",
    "    [[31, 32, 33], [34, 35, 36]],\n",
    "    [[37, 38, 39], [40, 41, 42]],\n",
    "    [[43, 44, 45], [46, 47, 48]],\n",
    "    [[49, 50, 51], [52, 53, 54]],\n",
    "    [[55, 56, 57], [58, 59, 60]]\n",
    "]\n",
    "\n",
    "# 使用 zip(*experiments_results) 获取每个实验对应位置的结果，然后对每个位置的结果求平均值\n",
    "averaged_results = [\n",
    "    [sum(values) / len(values) for values in zip(*experiment)]\n",
    "    for experiment in zip(*experiments_results)\n",
    "]\n",
    "\n",
    "print(averaged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2 = [[1,2],[4,5]]\n",
    "len(list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # 这里保证了self.conv1和self.bn1的参数在设备上\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False).to(device)\n",
    "        self.bn1 = nn.BatchNorm2d(64).to(device)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512*block.expansion, num_classes).to(device)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layer = block(self.in_planes, planes, stride)\n",
    "            layer = layer.to(device)  # 将子模块移动到设备上\n",
    "            layers.append(layer)\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out1 = out\n",
    "        out = self.layer2(out)\n",
    "        out2 = out\n",
    "        out = self.layer3(out)\n",
    "        out3 = out\n",
    "        out = self.layer4(out)\n",
    "        out4 = out\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return x, out1, out2, out3, out4, out\n",
    "    \n",
    "    def children(self):\n",
    "        # Return an iterator over child modules\n",
    "        return iter([self.layer1, self.layer2, self.layer3, self.layer4, self.avgpool])\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "# 对模型参数进行高斯分布的随机初始化\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        init.normal_(m.weight.data, mean=0, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias.data, 0)  # 如果有偏置，也进行初始化\n",
    "\n",
    "# 使用示例\n",
    "net = ResNet18()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(net)\n",
    "# 对模型的所有权重参数应用初始化\n",
    "net.to(device)\n",
    "net.apply(init_weights)\n",
    "\n",
    "\n",
    "# 示例输入\n",
    "sample_input = torch.randn(64, 3, 32, 32).to(device)  # 输入大小为(批量大小, 通道数, 高度, 宽度)\n",
    "\n",
    "# 前向传播\n",
    "original_input, hidden_output1, hidden_output2, hidden_output3, hidden_output4, output = net(sample_input)\n",
    "# print(\"Output shape:\", output.shape)\n",
    "# print(\"Original input shape:\", original_input.shape)\n",
    "# print(\"Hidden output shape:\", hidden_output1.shape)\n",
    "\n",
    "# 打印模型参数\n",
    "for name, param in net.named_parameters():\n",
    "    # print(name, param.data)\n",
    "    print(f\"Parameter '{name}' device: {param.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# 加载预训练的ResNet模型\n",
    "model = models.resnet50(pretrained=False)\n",
    "# model.apply(init_weights)\n",
    "\n",
    "# 获取模型的层级信息\n",
    "print(model)\n",
    "\n",
    "# 获取中间隐藏层的输出\n",
    "# 假设我们想要获取第4个卷积层的输出\n",
    "desired_layer = model.layer3[4]\n",
    "\n",
    "# 创建一个新的PyTorch模型，使其输出你所需的层的特征\n",
    "new_model = torch.nn.Sequential(\n",
    "    *list(model.children())[:8],  # 选择ResNet的前8层（包括你想要的隐藏层）\n",
    "    desired_layer\n",
    ")\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "new_model.eval()\n",
    "\n",
    "# 假设有输入数据x\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# 通过新模型获取隐藏层的输出\n",
    "hidden_output = new_model(x)\n",
    "print(hidden_output.shape)  # 输出隐藏层的特征张量的形状\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pprint import pprint\n",
    "# 读取.pkl文件\n",
    "file_path = r'.\\angle_layer_out\\2\\LeNet\\5\\betti_number.pkl'  # 替换成你的.pkl文件路径\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# 打印读取的内容\n",
    "pprint(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path2 = r'.\\angle_layer_out\\10\\ResNet152\\compare_different_bitte_norm_in_same_augmentation.pkl'  # 替换成你的.pkl文件路径\n",
    "with open(file_path2, 'rb') as file2:\n",
    "    data2 = pickle.load(file2)\n",
    "\n",
    "# 打印读取的内容\n",
    "pprint(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "x = random_array_1d = np.random.rand(100)  # 生成100个0到1之间的随机数\n",
    "y = random_array_1d = np.random.rand(100)  # 生成100个0到1之间的随机数\n",
    "z = random_array_1d = np.random.rand(100)  # 生成100个0到1之间的随机数\n",
    "# 创建 3D 图表\n",
    "fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, mode='lines')])\n",
    "\n",
    "# 保存为 HTML 文件\n",
    "pio.write_html(fig, file='3d_plot.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from net.custome_net import LeNet, MLP, ResNet18,ResNet34, ResNet50, ResNet101, ResNet152\n",
    "\n",
    "model = ResNet152()\n",
    "\n",
    "# 获取模型类名并打印\n",
    "model_name = type(model).__name__\n",
    "print(\"Model Name:\", model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.after_betti import after_get_bars\n",
    "\n",
    "after_get_bars(base_path=r\".\\angle_layer_out\\0\\LeNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def find_and_print_files(file_path: str, target_folder: str):\n",
    "    if not os.path.isdir(file_path):\n",
    "        print(\"Invalid directory path.\")\n",
    "        return\n",
    "\n",
    "    for root, dirs, files in os.walk(file_path):\n",
    "        # print(dirs)\n",
    "        if target_folder in dirs:\n",
    "            # print(target_folder)\n",
    "            target_folder_path = os.path.join(root, target_folder)\n",
    "            print(root.split(\"\\\\\")[-1])\n",
    "            pkl_file_path = os.path.join(target_folder_path, \"compare_different_bitte_norm_in_same_augmentation.pkl\")\n",
    "\n",
    "            if os.path.exists(pkl_file_path):\n",
    "                with open(pkl_file_path, 'rb') as pkl_file:\n",
    "                    try:\n",
    "                        obj = pickle.load(pkl_file)\n",
    "                        print(f\"File: {pkl_file_path}\")\n",
    "                        print(obj)  # Change this to whatever processing/printing you want to do with the loaded object\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {pkl_file_path}: {e}\")\n",
    "\n",
    "find_and_print_files(r\".\\angle_layer_out\", \"LeNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def collect_data_into_dict(file_path: str, target_folder: str) -> dict:\n",
    "    data_dict = {}\n",
    "\n",
    "    if not os.path.isdir(file_path):\n",
    "        print(\"Invalid directory path.\")\n",
    "        return data_dict\n",
    "\n",
    "    for root, dirs, files in os.walk(file_path):\n",
    "        if target_folder in dirs:\n",
    "            target_folder_path = os.path.join(root, target_folder)\n",
    "            pkl_file_path = os.path.join(target_folder_path, \"compare_different_bitte_norm_in_same_augmentation.pkl\")\n",
    "\n",
    "            if os.path.exists(pkl_file_path):\n",
    "                with open(pkl_file_path, 'rb') as pkl_file:\n",
    "                    try:\n",
    "                        obj = pickle.load(pkl_file)\n",
    "                        key = root.split(os.sep)[-1]\n",
    "                        data_dict[key] = obj\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {pkl_file_path}: {e}\")\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def restructure_dict(data_dict: dict) -> dict:\n",
    "    restructured_dict = {}\n",
    "    for key, value in data_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            for inner_key, inner_value in value.items():\n",
    "                if inner_key not in restructured_dict:\n",
    "                    restructured_dict[inner_key] = {}\n",
    "                restructured_dict[inner_key][key] = inner_value\n",
    "        else:\n",
    "            restructured_dict[key] = value\n",
    "    return restructured_dict\n",
    "\n",
    "\n",
    "def print_data_dict(data_dict: dict):\n",
    "    for key, value in data_dict.items():\n",
    "        pprint(f\"Key: {key}\")\n",
    "        pprint(f\"Value: {value}\")\n",
    "        pprint(\"\\n\")\n",
    "\n",
    "# Example usage:\n",
    "file_path = r\".\\angle_layer_out\"\n",
    "target_folder = \"LeNet\"\n",
    "\n",
    "data_dict = collect_data_into_dict(file_path, target_folder)\n",
    "pprint(data_dict)\n",
    "# restructured_data_dict = restructure_dict(data_dict)\n",
    "# df = pd.DataFrame(restructured_data_dict)\n",
    "df = pd.DataFrame(data_dict)\n",
    "print(df.shape)\n",
    "# print(df)\n",
    "print(df.transpose())\n",
    "df.transpose().to_pickle(f'{file_path}\\{target_folder}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path2 = r'.\\angle_layer_out\\LeNet.pkl'  # 替换成你的.pkl文件路径\n",
    "with open(file_path2, 'rb') as file2:\n",
    "    data2 = pickle.load(file2)\n",
    "\n",
    "# 打印读取的内容\n",
    "pprint(data2[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.check_betti4net import get_layer_output_betti\n",
    "import torchvision.transforms as transforms\n",
    "from net.custome_net import LeNet, MLP\n",
    "\n",
    "image_size = 32\n",
    "CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\n",
    "CIFAR_STD = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "model_LeNet = LeNet()\n",
    "angle_path = \"./angle_layer_out\"\n",
    "LeNet_path = f\"{angle_path}/LeNet/{0}\"\n",
    "data_transform={'train':transforms.Compose([\n",
    "                    transforms.RandomRotation(degrees=(0, 5)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "                    ])}\n",
    "get_layer_output_betti(model=model_LeNet, seed=15, save_root=LeNet_path, name=\"LeNet\", debug_size=1000, transform=data_transform[\"train\"])\n",
    "after_get_bars(base_path = LeNet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def read_pkl_files_in_folders(path):\n",
    "    pkl_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for folder in dirs:\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            pkl_file_path = os.path.join(folder_path, 'compare_different_bitte_norm_in_same_augmentation.pkl')\n",
    "            \n",
    "            if os.path.exists(pkl_file_path):\n",
    "                try:\n",
    "                    with open(pkl_file_path, 'rb') as file:\n",
    "                        data = pickle.load(file)\n",
    "                        print(f\"File: {pkl_file_path}\")\n",
    "                        print(data)  # 这里打印你读取到的数据\n",
    "                        pkl_list.append(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {pkl_file_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"No 'compare_different_bitte_norm_in_same_augmentation.pkl' found in {folder_path}\")\n",
    "    return pkl_list\n",
    "\n",
    "# 调用函数并传入文件路径\n",
    "folder_path_to_search = './angle_layer_out/LeNet/'  # 替换为你的文件路径\n",
    "pkl_list = read_pkl_files_in_folders(folder_path_to_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pkl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "def merge_ordered_dicts(list_of_dicts):\n",
    "    merged_dict = OrderedDict()\n",
    "\n",
    "    for current_dict in list_of_dicts:\n",
    "        nested_dict = merged_dict\n",
    "        for key, value in current_dict.items():\n",
    "            if key in nested_dict:\n",
    "                if isinstance(nested_dict[key], dict) and isinstance(value, dict):\n",
    "                    # 如果当前键已存在且对应的值都是字典，则递归合并\n",
    "                    nested_dict[key] = merge_ordered_dicts([nested_dict[key], value])\n",
    "                else:\n",
    "                    # 否则，将值更新为一个列表，包含之前的值和新的值\n",
    "                    nested_dict[key] = [nested_dict[key], value]\n",
    "            else:\n",
    "                nested_dict[key] = value\n",
    "\n",
    "    return merged_dict\n",
    "\n",
    "# 示例用法\n",
    "list_of_ordered_dicts = [\n",
    "    OrderedDict([('key1', OrderedDict([('subkey1', 'value1')])),\n",
    "                 ('key2', 'value2')]),\n",
    "    OrderedDict([('key1', OrderedDict([('subkey1', 'value3')])),\n",
    "                 ('key2', 'value4')])\n",
    "]\n",
    "\n",
    "result = merge_ordered_dicts(pkl_list)\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def split_ordered_dict(input_dict):\n",
    "    # 获取前两层的keys\n",
    "    first_level_keys = list(input_dict.keys())\n",
    "    second_level_keys = list(input_dict[first_level_keys[0]].keys())\n",
    "\n",
    "    # 创建15个新的空字典\n",
    "    new_dicts = [{} for _ in range(len(first_level_keys) * len(second_level_keys))]\n",
    "\n",
    "    # 遍历原字典前两层，将值填充到新字典中\n",
    "    index = 0\n",
    "    for first_key in first_level_keys:\n",
    "        for second_key in second_level_keys:\n",
    "            new_key = first_key + \"_\" + second_key\n",
    "            new_dicts[index][new_key] = input_dict[first_key][second_key]\n",
    "            index += 1\n",
    "\n",
    "    return new_dicts\n",
    "\n",
    "# 示例用法\n",
    "input_ordered_dict = {\n",
    "    'A': {'1': {'x': 10, 'y': 20}, '2': {'x': 30, 'y': 40}},\n",
    "    'B': {'1': {'x': 50, 'y': 60}, '2': {'x': 70, 'y': 80}},\n",
    "    'C': {'1': {'x': 90, 'y': 100}}\n",
    "}\n",
    "\n",
    "result2 = split_ordered_dict(result)\n",
    "for d in result2:\n",
    "    print(d)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_3d_array(input_dict):\n",
    "    result_dict = {}\n",
    "    for main_key, inner_dict in input_dict.items():\n",
    "        result = []\n",
    "        for key, value in inner_dict.items():\n",
    "            new_key = tuple(map(int, key.split('\\\\')[1:])) + (value,)\n",
    "            result.append(new_key)\n",
    "        result_dict[main_key] = result\n",
    "    return result_dict\n",
    "\n",
    "# 示例用法\n",
    "input_dict = {\n",
    "    'L1-B0_bar_number': OrderedDict([\n",
    "        ('LeNet\\\\0\\\\0', 960),\n",
    "        ('LeNet\\\\0\\\\1', 960),\n",
    "        ('LeNet\\\\1\\\\0', 960),\n",
    "    ])\n",
    "}\n",
    "\n",
    "# print(input_dict)\n",
    "result3 = convert_dict_to_3d_array(result2[0])\n",
    "print(result3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "\n",
    "def plot_3d_interactive(input_dict, file_path):\n",
    "    # 检查文件夹是否存在，不存在则创建\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for key, data in input_dict.items():\n",
    "        x, y, z = zip(*data)\n",
    "        fig.add_trace(go.Scatter3d(x=x, y=y, z=z, mode='markers', name=key))\n",
    "\n",
    "    fig.update_layout(scene=dict(\n",
    "                        xaxis=dict(title='增强的强度'),\n",
    "                        yaxis=dict(title='输出的层数'),\n",
    "                        zaxis=dict(title='指标的大小')),\n",
    "                      title=list(input_dict.keys())[0])\n",
    "\n",
    "    file_name = os.path.join(file_path, list(input_dict.keys())[0] + \".html\")\n",
    "    fig.write_html(file_name)\n",
    "\n",
    "# 示例用法\n",
    "input_data = {\n",
    "    'L1-B0_bar_number': [\n",
    "        (0, 0, 960),\n",
    "        (0, 1, 960),\n",
    "        (0, 2, 960),\n",
    "        (0, 3, 960)\n",
    "    ]\n",
    "}\n",
    "\n",
    "file_save_path = '/your/save/path/'  # 请替换为你想要保存的路径\n",
    "plot_3d_interactive(input_data, \"./net/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_subfolders(path: str) -> None:\n",
    "    \"\"\"\n",
    "    Retrieves subfolder paths and visualizes data from pkl files in each subfolder.\n",
    "\n",
    "    Args:\n",
    "    - path (str): The root path.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    subfolders = [os.path.join(path, folder) for folder in os.listdir(path) if os.path.isdir(os.path.join(path, folder))]\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        visualize_aug_layers(subfolder)\n",
    "        print(subfolder)\n",
    "\n",
    "get_subfolders(\"./angle_layer_out/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def replace_indexes_with_values(index_tuple: Tuple[int, ...], value_list: List[int]) -> Tuple[int, ...]:\n",
    "    \"\"\"\n",
    "    Replaces indexes in a tuple with corresponding values from a list.\n",
    "\n",
    "    Args:\n",
    "    - index_tuple (Tuple[int, ...]): Tuple containing indexes.\n",
    "    - value_list (List[int]): List containing values.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[int, ...]: Tuple with indexes replaced by values.\n",
    "    \"\"\"\n",
    "    replaced_values = tuple(value_list[idx] for idx in index_tuple)\n",
    "    return replaced_values\n",
    "\n",
    "index_tuple = (0, 2, 1)\n",
    "value_list = [\"10\", \"20\", \"30\"]\n",
    "\n",
    "result = replace_indexes_with_values(index_tuple, value_list)\n",
    "print(result)  # Output: (10, 30, 20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchGpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
