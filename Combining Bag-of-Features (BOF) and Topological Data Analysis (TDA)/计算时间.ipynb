{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算BOF和TDA所需要的时间"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些准备工作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个计算运行时间的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def coumpte_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Function {func.__name__} took {execution_time} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个张量列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 创建一个空列表用于存放张量\n",
    "tensor_list = []\n",
    "\n",
    "# 定义张量形状\n",
    "tensor_shape = (50000, 32*32*3)\n",
    "\n",
    "# 生成5000个符合高斯分布的张量并添加到列表中\n",
    "for _ in range(1):\n",
    "    # 使用torch.randn生成符合标准正态分布的张量\n",
    "    tensor = torch.randn(tensor_shape)\n",
    "    tensor_list.append(tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算BOF消耗的时间"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一个计算BOF的实例函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BOF.get_rank_from_matrix import Effective_Ranks\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def get_BOF(tensor_list:list = []):\n",
    "    # 现在tensor_list里面有5000个符合高斯分布的张量，每个张量形状为(5000, 32*32*3)\n",
    "        \n",
    "\n",
    "    results = []\n",
    "    for image_matrix in tensor_list:\n",
    "        # 检查张量形状\n",
    "        if image_matrix.size(0) > image_matrix.size(1):\n",
    "            image_matrix = image_matrix.T  # 如果 m > n，执行转置操作\n",
    "        \n",
    "        get_rank = Effective_Ranks(image_matrix)\n",
    "        r0 = get_rank.r0\n",
    "        R0 = get_rank.R0\n",
    "        rk_max_index = get_rank.rk_max_index\n",
    "        rk_max = get_rank.rk_max_value\n",
    "        Rk_max = get_rank.Rk_value_max_rk_index\n",
    "\n",
    "        results.append({\"isic\": {\"r0\": r0, \"R0\": R0, \"rk_max_index\": rk_max_index, \"rk_max\": rk_max, \"Rk_max\": Rk_max}})\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072, 50000]) torch.float32\n",
      "(3072, 3072) float64\n",
      "Function get_BOF took 8.636544466018677 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'isic': {'r0': 1973.026815189157,\n",
       "   'R0': 2894.0202108206254,\n",
       "   'rk_max_index': 52,\n",
       "   'rk_max': 2001.675971508667,\n",
       "   'Rk_max': 2852.1083901756538}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_BOF()\n",
    "# 使用function_A包装function_B\n",
    "timed_function_B = coumpte_time(get_BOF)\n",
    "\n",
    "# 调用timed_function_B，并传入参数\n",
    "timed_function_B(tensor_list)  # 传入函数B所需的参数\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算TDA需要的时间"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一个计算TDA的实例函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import ripser\n",
    "\n",
    "def get_TDA(tensor_list:list = []):\n",
    "\n",
    "    # 现在tensor_list里面有5000个符合高斯分布的张量，每个张量形状为(5000, 32*32*3)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for image_matrix in tensor_list:\n",
    "        TDA = ripser(X=image_matrix, maxdim=1)\n",
    "        results.append(TDA['dgms'])\n",
    "\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 37.3 GiB for an array with shape (2499998288, 2) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m timed_function_B \u001b[39m=\u001b[39m coumpte_time(get_TDA)\n\u001b[0;32m      5\u001b[0m \u001b[39m# 调用timed_function_B，并传入参数\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m timed_function_B(tensor_list)  \u001b[39m# 传入函数B所需的参数\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 6\u001b[0m, in \u001b[0;36mcoumpte_time.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m      5\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m      7\u001b[0m     end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      8\u001b[0m     execution_time \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[35], line 10\u001b[0m, in \u001b[0;36mget_TDA\u001b[1;34m(tensor_list)\u001b[0m\n\u001b[0;32m      7\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m image_matrix \u001b[39min\u001b[39;00m tensor_list:\n\u001b[1;32m---> 10\u001b[0m     TDA \u001b[39m=\u001b[39m ripser(X\u001b[39m=\u001b[39;49mimage_matrix, maxdim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     11\u001b[0m     results\u001b[39m.\u001b[39mappend(TDA[\u001b[39m'\u001b[39m\u001b[39mdgms\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32md:\\anaconda\\envs\\PyTorchGpu\\Lib\\site-packages\\ripser\\ripser.py:311\u001b[0m, in \u001b[0;36mripser\u001b[1;34m(X, maxdim, thresh, coeff, distance_matrix, do_cocycles, metric, n_perm)\u001b[0m\n\u001b[0;32m    305\u001b[0m n_points \u001b[39m=\u001b[39m dm\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    306\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(dm) \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mabs(dm\u001b[39m.\u001b[39mdiagonal()) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    307\u001b[0m     \u001b[39m# If any of the diagonal elements are nonzero,\u001b[39;00m\n\u001b[0;32m    308\u001b[0m     \u001b[39m# convert to sparse format, because currently\u001b[39;00m\n\u001b[0;32m    309\u001b[0m     \u001b[39m# that's the only format that handles nonzero\u001b[39;00m\n\u001b[0;32m    310\u001b[0m     \u001b[39m# births\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     dm \u001b[39m=\u001b[39m sparse\u001b[39m.\u001b[39;49mcoo_matrix(dm)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(dm):\n\u001b[0;32m    314\u001b[0m     \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39misspmatrix_coo(dm):\n\u001b[0;32m    315\u001b[0m         \u001b[39m# If the matrix is already COO, we need to order the row and column indices\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         \u001b[39m# lexicographically to avoid errors. See issue #103\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\PyTorchGpu\\Lib\\site-packages\\scipy\\sparse\\_coo.py:195\u001b[0m, in \u001b[0;36m_coo_base.__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39minconsistent shapes: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m != \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    193\u001b[0m                          (shape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape))\n\u001b[0;32m    194\u001b[0m index_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_index_dtype(maxval\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape))\n\u001b[1;32m--> 195\u001b[0m row, col \u001b[39m=\u001b[39m M\u001b[39m.\u001b[39;49mnonzero()\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrow \u001b[39m=\u001b[39m row\u001b[39m.\u001b[39mastype(index_dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    197\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol \u001b[39m=\u001b[39m col\u001b[39m.\u001b[39mastype(index_dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 37.3 GiB for an array with shape (2499998288, 2) and data type int64"
     ]
    }
   ],
   "source": [
    "# get_BOF()\n",
    "# 使用function_A包装function_B\n",
    "timed_function_B = coumpte_time(get_TDA)\n",
    "\n",
    "# 调用timed_function_B，并传入参数\n",
    "timed_function_B(tensor_list)  # 传入函数B所需的参数\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算模型参数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义一个计算参数数量的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(models):\n",
    "    parameters_count = []\n",
    "\n",
    "    for model in models:\n",
    "        # print(type(model).__name__)\n",
    "        parameters_count.append(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "    return parameters_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算新ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1707274, 136886, 11173962, 21282122, 23520842, 42512970, 58156618]\n"
     ]
    }
   ],
   "source": [
    "from nets.resnet import ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\n",
    "from nets.simple_net import MLP, LeNet\n",
    "\n",
    "model_list = [MLP(), LeNet(), ResNet18(), ResNet34(), ResNet50(), ResNet101(), ResNet152()]\n",
    "\n",
    "para_list = count_parameters(model_list)\n",
    "print(para_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算旧ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1707274, 136886, 11173962, 21282122, 21282122, 41353546, 57883978]\n"
     ]
    }
   ],
   "source": [
    "from nets.simple_net import ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\n",
    "from nets.simple_net import MLP, LeNet\n",
    "\n",
    "model_list = [MLP(), LeNet(), ResNet18(), ResNet34(), ResNet50(), ResNet101(), ResNet152()]\n",
    "\n",
    "para_list = count_parameters(model_list)\n",
    "print(para_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchGpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
